# CSSR Statistics

This module contains code which, in conjunction with CSSR.Inferred, is used to evaluate CSSR models. This readme is a short breakdown and explaination of how this all works, as well as a brief description of the metrics used.

---

### CSSR.Inferred

This module takes the CSSR model generated (i.e. the terminal nodes) and constructs the inferred distribution for all max lengths of the parse tree. For cosma and kristi: why the max length in particular? In other words, given a max-length over all time, represented by the parse-tree, what is the probability that these histories could be generated by our discovered model?

---

### CSSR.Statistics.Entropy.Rate

> Information entropy is defined as the average amount of information produced by a stochastic source of data.
> The basic model of a data communication system is composed of three elements, a source of data, a channel, and a receiver, and the "fundamental problem of communication" is for the receiver to be able to identify what data was generated by the source, based on the signal it receives through the channel.

Entropy is a way to describe IID processes which is the joint entropy over all distributions of all subsets
Entropy rate (also the _source information rate_) answers the question, "how does the entropy of the sequency grow with n."[0] This winds up being the average as n approaches infinity.

[0]:http://poincare.matf.bg.ac.rs/nastavno/viktor/Entropy_Rates_of_a_Stochastic_Process.pdf

---

wikipedia on implications of MC in entropy rates:

> Since a stochastic process defined by a Markov chain that is irreducible, aperiodic and positive recurrent has a stationary distribution, the entropy rate is independent of the initial distribution.
> ...
> A simple consequence of this definition is that an iid (independent, identically distributed) stochastic process has an entropy rate that is the same as the entropy of any individual member of the process (via independence). - https://en.wikipedia.org/wiki/Entropy_rate

citation needed.

*Stationary process*: a stochastic process is stationary if the join distribution of any subset is invariant to time-shift. IE: p(heads_1, tails_2) == p (tails_2, heads_3)

---

### CSSR.Statistics.Entropy.Relative

Also known as the KL (Kullback–Leibler) divergence, which is used _everywhere_.

> is a measure of how one probability distribution diverges from a second, expected probability distribution.wiki-1 wiki-2 Applications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference.

> it is a distribution-wise asymmetric measure and does not qualify as a statistical metric of spread.

(*distribution-wise* - varies by distributions)
(*asymmetric measure* - basically implies that good and bad are on the left and right)

A Kullback–Leibler divergence of 0 indicates similar distributions, while a divergence of 1 implies that the expectation given the first distribution approaches zero in the second.

[wiki-erel]: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence
[1]: see_wikipedia
[2]: see_wikipedia

---

### CSSR.Statistics.Entropy.RelativeRate

Much like Entropy rate allows us to scale Entropy from a single observation to a sequential process, relative entropy rate does so as well.

[if,p33]: https://ee.stanford.edu/~gray/it.pdf

---

### CSSR.Statistics.F1

(F-score, F-measure) basically a combined measure of precision (number of correct positive results / number of all positive results) and recall (number of correct positive results / number of positive results that should have been returned).

> The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. -- https://en.wikipedia.org/wiki/F1_score

*harmonic mean*: (subcontrary mean) is appropriate for situations when the average of rates is desired. one of many kinds of means.

---

### CSSR.Statistics.CMu

statistical complexity:

> the informational size of the distribution over causal states, as measured by the Shannon entropy, gives the minimum average amount of memory needed to optimally predict the right-half configurations.  This quantity is the statistical complexity -- http://hornacek.coa.edu/dave/Tutorial/notes.pdf

http://csc.ucdavis.edu/~cmg/papers/mscw.pdf
http://hornacek.coa.edu/dave/Tutorial/

---

### CSSR.Statistics.Entropy.Variation

I think is [this][wiki-var]:

> variation of information or shared information distance is a measure of the distance between two clusterings (partitions of elements). It is closely related to mutual information; indeed, it is a simple linear expression involving the mutual information. Unlike the mutual information, however, the variation of information is a true metric, in that it obeys the triangle inequality.

[wiki-var]:https://en.wikipedia.org/wiki/Variation_of_information

